{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here's a point-wise explanation for each of your 20 questions on **K-Nearest Neighbors (KNN)** and **Principal Component Analysis (PCA)** â€” ideal for concept clarity and exam prep:\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”µ **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "**1. What is K-Nearest Neighbors (KNN) and how does it work?**\n",
        "\n",
        "* KNN is a **non-parametric, lazy learning algorithm** used for classification and regression.\n",
        "* It **predicts the output** for a sample based on the **'K' closest training samples** (neighbors).\n",
        "* Closeness is measured using a **distance metric** (like Euclidean distance).\n",
        "\n",
        "---\n",
        "\n",
        "**2. Difference between KNN Classification and KNN Regression:**\n",
        "\n",
        "| KNN Classification              | KNN Regression                    |\n",
        "| ------------------------------- | --------------------------------- |\n",
        "| Predicts a **class label**      | Predicts a **continuous value**   |\n",
        "| Majority vote among neighbors   | Average (mean) of neighbor values |\n",
        "| Suitable for categorical output | Suitable for numerical output     |\n",
        "\n",
        "---\n",
        "\n",
        "**3. Role of the distance metric in KNN:**\n",
        "\n",
        "* It determines **how closeness is measured** between data points.\n",
        "* Common metrics:\n",
        "\n",
        "  * **Euclidean distance** (default)\n",
        "  * **Manhattan distance**\n",
        "  * **Minkowski distance**\n",
        "* The choice affects accuracy, especially with different feature scales.\n",
        "\n",
        "---\n",
        "\n",
        "**4. What is the Curse of Dimensionality in KNN?**\n",
        "\n",
        "* In high dimensions, all points appear **equally distant**.\n",
        "* This reduces KNN's effectiveness as it **relies on distance** for prediction.\n",
        "* Dimensionality reduction (like PCA) can help mitigate this.\n",
        "\n",
        "---\n",
        "\n",
        "**5. How can we choose the best value of K in KNN?**\n",
        "\n",
        "* Use **cross-validation** to test different K values.\n",
        "* **Odd values** are preferred (to avoid ties in classification).\n",
        "* Plot **error rate vs. K** to find the **elbow point**.\n",
        "\n",
        "---\n",
        "\n",
        "**6. What are KD Tree and Ball Tree in KNN?**\n",
        "\n",
        "* They are **data structures** that help **optimize neighbor search**:\n",
        "\n",
        "  * **KD Tree**: Binary tree partitioning the data space.\n",
        "  * **Ball Tree**: Uses hyperspheres to group points.\n",
        "* Speeds up query time from O(n) to O(log n) in many cases.\n",
        "\n",
        "---\n",
        "\n",
        "**7. When should you use KD Tree vs. Ball Tree?**\n",
        "\n",
        "* **KD Tree**: Efficient for **low-dimensional** data (<20 features).\n",
        "* **Ball Tree**: Better for **higher-dimensional** data.\n",
        "* Scikit-learn chooses automatically based on dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**8. Disadvantages of KNN:**\n",
        "\n",
        "* **Slow prediction** (lazy learner).\n",
        "* **High memory usage** (stores all data).\n",
        "* **Sensitive to noise** and irrelevant features.\n",
        "* **Performance drops** with high dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "**9. How does feature scaling affect KNN?**\n",
        "\n",
        "* Distance-based algorithms are **very sensitive to feature scales**.\n",
        "* Always apply **standardization or normalization** before KNN.\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŸ  **Principal Component Analysis (PCA)**\n",
        "\n",
        "**10. What is PCA (Principal Component Analysis)?**\n",
        "\n",
        "* PCA is a **dimensionality reduction** technique.\n",
        "* It transforms data into a new coordinate system using **orthogonal components** (principal components).\n",
        "\n",
        "---\n",
        "\n",
        "**11. How does PCA work?**\n",
        "\n",
        "1. **Standardize** the data\n",
        "2. Compute the **covariance matrix**\n",
        "3. Find **eigenvectors and eigenvalues**\n",
        "4. Choose top-k eigenvectors\n",
        "5. Transform the data onto the new axes\n",
        "\n",
        "---\n",
        "\n",
        "**12. Geometric intuition behind PCA:**\n",
        "\n",
        "* PCA finds new **axes (principal components)** that **maximize variance**.\n",
        "* The first component captures the **most variance**, the second captures the next most, **orthogonal to the first**, and so on.\n",
        "\n",
        "---\n",
        "\n",
        "**13. Difference between Feature Selection and Feature Extraction:**\n",
        "\n",
        "| Feature Selection            | Feature Extraction              |\n",
        "| ---------------------------- | ------------------------------- |\n",
        "| Keeps original features      | Creates new features            |\n",
        "| Removes unimportant features | Transforms features (e.g., PCA) |\n",
        "| Example: Chi-square, ANOVA   | Example: PCA, LDA               |\n",
        "\n",
        "---\n",
        "\n",
        "**14. What are Eigenvalues and Eigenvectors in PCA?**\n",
        "\n",
        "* **Eigenvectors**: Directions (axes) of new feature space.\n",
        "* **Eigenvalues**: Magnitude of variance along each eigenvector.\n",
        "* Larger eigenvalue = more important component.\n",
        "\n",
        "---\n",
        "\n",
        "**15. How do you decide the number of components to keep in PCA?**\n",
        "\n",
        "* Use **explained variance ratio**.\n",
        "* Keep components that together explain **90â€“95% variance**.\n",
        "* Use a **scree plot** to visualize the drop-off (elbow method).\n",
        "\n",
        "---\n",
        "\n",
        "**16. Can PCA be used for classification?**\n",
        "\n",
        "* **Indirectly**. PCA reduces dimensionality, which can:\n",
        "\n",
        "  * Improve model performance\n",
        "  * Reduce overfitting\n",
        "* But PCA itself is **unsupervised**, not a classifier.\n",
        "\n",
        "---\n",
        "\n",
        "**17. What are the limitations of PCA?**\n",
        "\n",
        "* Assumes **linear relationships**.\n",
        "* Sensitive to **outliers**.\n",
        "* Reduced interpretability â€” components are **linear combinations** of features.\n",
        "* May discard important but **low-variance** features.\n",
        "\n",
        "---\n",
        "\n",
        "**18. How do KNN and PCA complement each other?**\n",
        "\n",
        "* PCA reduces dimensions â†’ **improves KNN performance**.\n",
        "* Reduces **overfitting**, **noise**, and **computational cost** in KNN.\n",
        "* Useful before applying KNN to high-dimensional data.\n",
        "\n",
        "---\n",
        "\n",
        "**19. How does KNN handle missing values in a dataset?**\n",
        "\n",
        "* **KNN doesnâ€™t handle missing values natively.**\n",
        "* Preprocessing required:\n",
        "\n",
        "  * Use **KNN imputation**\n",
        "  * Or remove/replace missing values beforehand\n",
        "\n",
        "---\n",
        "\n",
        "**20. Key differences between PCA and LDA:**\n",
        "\n",
        "| PCA                           | LDA                             |\n",
        "| ----------------------------- | ------------------------------- |\n",
        "| Unsupervised                  | Supervised                      |\n",
        "| Maximizes **variance**        | Maximizes **class separation**  |\n",
        "| Doesn't consider labels       | Uses class labels               |\n",
        "| Can be used for visualization | Better for classification tasks |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a **PDF or revision sheet** for this set as well? I can generate that for your study materials.\n"
      ],
      "metadata": {
        "id": "4hM5hS7-b8Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Train a KNN Classifier on the Iris dataset and print model accuracy.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train KNN Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = knn_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"KNN Classifier Accuracy: {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v255qyub8B7",
        "outputId": "c4709e92-e05d-49da-aee9-3460dab82d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNjP5o2sbTzW",
        "outputId": "43c981f9-6546-478d-a4ff-1290e628383c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Regressor Mean Squared Error: 579.5921\n"
          ]
        }
      ],
      "source": [
        "# 22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE).\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train KNN Regressor\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MSE\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print MSE\n",
        "print(\"KNN Regressor Mean Squared Error: {:.4f}\".format(mse))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Test different distance metrics\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "for metric in metrics:\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    y_pred = knn_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"KNN Classifier ({metric} distance) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5YsPUKKckVa",
        "outputId": "bd1abdae-aebb-41cb-d2a4-c332cb80df02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier (euclidean distance) Accuracy: 1.0000\n",
            "KNN Classifier (manhattan distance) Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Train a KNN Classifier with different values of K and visualize decision boundaries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset (use only first two features for 2D visualization)\n",
        "data = load_iris()\n",
        "X, y = data.data[:, :2], data.target\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define k values\n",
        "k_values = [1, 5, 10]\n",
        "\n",
        "# Plot decision boundaries\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, k in enumerate(k_values, 1):\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_clf.fit(X, y)\n",
        "\n",
        "    # Create mesh grid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    # Predict on mesh grid\n",
        "    Z = knn_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot\n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='viridis')\n",
        "    plt.title(f'KNN Decision Boundary (k={k})')\n",
        "    plt.xlabel('Feature 1 (Scaled)')\n",
        "    plt.ylabel('Feature 2 (Scaled)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('knn_decision_boundaries.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "U8n8B86-cqIM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Apply Feature Scaling before training a KNN model and compare results with unscaled data.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN without scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN with scaling\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"KNN Classifier Accuracy (Unscaled): {accuracy_unscaled:.4f}\")\n",
        "print(f\"KNN Classifier Accuracy (Scaled): {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwyE8aBccubL",
        "outputId": "9d6cc8f0-4c09-48ca-c811-b22d86bb00ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier Accuracy (Unscaled): 1.0000\n",
            "KNN Classifier Accuracy (Scaled): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Train a PCA model on synthetic data and print the explained variance ratio for each component.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio for Each Component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
        "    print(f\"Component {i}: {var:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PwFzuhjc8_1",
        "outputId": "c43fbc63-032b-4651-caa5-2a8a665c062b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio for Each Component:\n",
            "Component 1: 0.2992\n",
            "Component 2: 0.1560\n",
            "Component 3: 0.1112\n",
            "Component 4: 0.1029\n",
            "Component 5: 0.0987\n",
            "Component 6: 0.0959\n",
            "Component 7: 0.0884\n",
            "Component 8: 0.0475\n",
            "Component 9: 0.0000\n",
            "Component 10: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN without PCA\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "accuracy_no_pca = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN with PCA\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"KNN Classifier Accuracy (No PCA): {accuracy_no_pca:.4f}\")\n",
        "print(f\"KNN Classifier Accuracy (PCA, 2 components): {accuracy_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB4ASQE-dBYa",
        "outputId": "d841ead1-4f08-46a2-d5c3-add6ee437003"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier Accuracy (No PCA): 1.0000\n",
            "KNN Classifier Accuracy (PCA, 2 components): 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize KNN Classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'n_neighbors': [3, 5, 7, 9], 'metric': ['euclidean', 'manhattan']}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and predict\n",
        "best_knn = grid_search.best_estimator_\n",
        "y_pred = best_knn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"KNN Classifier Accuracy (Tuned): {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wkfYxzJdIST",
        "outputId": "38d87be7-00b3-4bb5-c528-2b8c62bac62c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'metric': 'manhattan', 'n_neighbors': 9}\n",
            "KNN Classifier Accuracy (Tuned): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Train a KNN Classifier and check the number of misclassified samples.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict and count misclassified samples\n",
        "y_pred = knn.predict(X_test)\n",
        "misclassified = np.sum(y_pred != y_test)\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of Misclassified Samples: {misclassified}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAMp0_4udODz",
        "outputId": "c9a68a4f-062f-4d6d-9341-59b748d16c0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Misclassified Samples: 0\n",
            "Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. Train a PCA model and visualize the cumulative explained variance.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', color='#1f77b4')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.title('PCA Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.savefig('pca_cumulative_variance.png')\n",
        "plt.close()  # Close the figure to free up memory"
      ],
      "metadata": {
        "id": "5KVd1wysdSll"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Test different weights\n",
        "weights_list = ['uniform', 'distance']\n",
        "\n",
        "for weight in weights_list:\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=5, weights=weight)\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    y_pred = knn_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"KNN Classifier (weights={weight}) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dvKwSidaFJ",
        "outputId": "5261da89-fe35-49f1-ace5-da52a02dbc42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier (weights=uniform) Accuracy: 1.0000\n",
            "KNN Classifier (weights=distance) Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32. Train a KNN Regressor and analyze the effect of different K values on performance.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Test different k values\n",
        "k_values = [1, 5, 10, 20]\n",
        "\n",
        "for k in k_values:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_reg.fit(X_train, y_train)\n",
        "    y_pred = knn_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"KNN Regressor (k={k}) Mean Squared Error: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM_pUlSkdp3j",
        "outputId": "afc3b565-d8d2-4b63-dda1-4e9d716991d1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Regressor (k=1) Mean Squared Error: 1209.4019\n",
            "KNN Regressor (k=5) Mean Squared Error: 579.5921\n",
            "KNN Regressor (k=10) Mean Squared Error: 616.3125\n",
            "KNN Regressor (k=20) Mean Squared Error: 807.2294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 33. Implement KNN Imputation for handling missing values in a dataset.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=4, random_state=42)\n",
        "\n",
        "# Introduce missing values (10% of data)\n",
        "rng = np.random.RandomState(42)\n",
        "missing_mask = rng.random(X.shape) < 0.1\n",
        "X[missing_mask] = np.nan\n",
        "\n",
        "# Apply KNN Imputation\n",
        "imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Check number of missing values before and after\n",
        "missing_before = np.isnan(X).sum()\n",
        "missing_after = np.isnan(X_imputed).sum()\n",
        "\n",
        "# Print results\n",
        "print(f\"Missing Values Before Imputation: {missing_before}\")\n",
        "print(f\"Missing Values After Imputation: {missing_after}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY1HWfqudvSx",
        "outputId": "65649e3d-343b-4709-92ac-5aae33572359"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values Before Imputation: 426\n",
            "Missing Values After Imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 34. Train a PCA model and visualize the data projection onto the first two principal components.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Visualize projection\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Projection onto First Two Components')\n",
        "plt.colorbar(scatter, label='Class')\n",
        "plt.savefig('pca_projection.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "vbmLLNbudz8a",
        "outputId": "b45b78f5-de1c-4353-c092-c087ea4ea71a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "n_classes(3) * n_clusters_per_class(2) must be smaller or equal 2**n_informative(2)=4",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-581120d1f933>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Generate synthetic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Scale the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_samples_generator.py\u001b[0m in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"n_classes({}) * n_clusters_per_class({}) must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" smaller or equal 2**n_informative({})={}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters_per_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_informative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mn_informative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: n_classes(3) * n_clusters_per_class(2) must be smaller or equal 2**n_informative(2)=4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Test different algorithms\n",
        "algorithms = ['kd_tree', 'ball_tree']\n",
        "\n",
        "for algo in algorithms:\n",
        "    start_time = time.time()\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    y_pred = knn_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"KNN Classifier ({algo}) Accuracy: {accuracy:.4f}, Training Time: {training_time:.4f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qcLuybieDLj",
        "outputId": "89fdf4bb-2091-47be-cb51-8a3cb394b565"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier (kd_tree) Accuracy: 1.0000, Training Time: 0.0009s\n",
            "KNN Classifier (ball_tree) Accuracy: 1.0000, Training Time: 0.0013s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Generate high-dimensional synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Plot Scree plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', color='#1f77b4')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('PCA Scree Plot')\n",
        "plt.grid(True)\n",
        "plt.savefig('pca_scree_plot.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "TCNRV9S_eKks"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37. Train a KNN Classifier and evaluate using Precision, Recall, and F1-score.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate metrics\n",
        "y_pred = knn_clf.predict(X_test)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Print results\n",
        "print(\"KNN Classifier Performance:\")\n",
        "print(f\"Precision (macro): {precision:.4f}\")\n",
        "print(f\"Recall (macro): {recall:.4f}\")\n",
        "print(f\"F1-Score (macro): {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BX6QlixePtT",
        "outputId": "69f11fa1-7f12-42d3-cddb-54cdd75dde7e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier Performance:\n",
            "Precision (macro): 1.0000\n",
            "Recall (macro): 1.0000\n",
            "F1-Score (macro): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 38. Train a PCA model and analyze the effect of different numbers of components on accuracy.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Test different numbers of PCA components\n",
        "n_components_list = [1, 2, 3, 4]\n",
        "\n",
        "for n in n_components_list:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_clf.fit(X_train_pca, y_train)\n",
        "    y_pred = knn_clf.predict(X_test_pca)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"KNN Classifier (PCA, {n} components) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VZ3vQSReYwj",
        "outputId": "8e51f95e-edc7-43fe-b0a4-d41dd0cb7de9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier (PCA, 1 components) Accuracy: 0.9000\n",
            "KNN Classifier (PCA, 2 components) Accuracy: 0.9333\n",
            "KNN Classifier (PCA, 3 components) Accuracy: 1.0000\n",
            "KNN Classifier (PCA, 4 components) Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 39. Train a KNN Classifier with different leaf_size values and compare accuracy.\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Test different leaf_size values\n",
        "leaf_sizes = [10, 30, 50]\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree', leaf_size=leaf_size)\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    y_pred = knn_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"KNN Classifier (leaf_size={leaf_size}) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUQGmxUCeeCc",
        "outputId": "40611fe5-988a-4330-d0ff-f765af64d6bb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier (leaf_size=10) Accuracy: 1.0000\n",
            "KNN Classifier (leaf_size=30) Accuracy: 1.0000\n",
            "KNN Classifier (leaf_size=50) Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Train a PCA model and visualize how data points are transformed before and after PCA.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset (use first two features for visualization)\n",
        "data = load_iris()\n",
        "X, y = data.data[:, :2], data.target\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Visualize before and after PCA\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
        "plt.xlabel('Feature 1 (Scaled)')\n",
        "plt.ylabel('Feature 2 (Scaled)')\n",
        "plt.title('Before PCA')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('After PCA')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pca_transformation.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "-jkgMZ-keimZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print classification report\n",
        "y_pred = knn_clf.predict(X_test)\n",
        "print(\"KNN Classifier Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkFj6hk7emwC",
        "outputId": "72c631c7-86f5-4e22-ed38-9773e68ea668"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       0.93      1.00      0.97        14\n",
            "     class_1       1.00      0.86      0.92        14\n",
            "     class_2       0.89      1.00      0.94         8\n",
            "\n",
            "    accuracy                           0.94        36\n",
            "   macro avg       0.94      0.95      0.94        36\n",
            "weighted avg       0.95      0.94      0.94        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Test different distance metrics\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "for metric in metrics:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn_reg.fit(X_train, y_train)\n",
        "    y_pred = knn_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"KNN Regressor ({metric} distance) Mean Squared Error: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxNHfkeuesxD",
        "outputId": "0b10f747-0e68-466f-ef67-a3a8383fe7c6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Regressor (euclidean distance) Mean Squared Error: 579.5921\n",
            "KNN Regressor (manhattan distance) Mean Squared Error: 627.7911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 43. Train a KNN Classifier and evaluate using ROC-AUC score.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and calculate ROC-AUC\n",
        "y_pred_proba = knn_clf.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print ROC-AUC\n",
        "print(\"KNN Classifier ROC-AUC Score: {:.4f}\".format(auc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wuy_pM4XexTr",
        "outputId": "ed112cdc-0c42-4c3e-8a3b-1ac7304e364e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier ROC-AUC Score: 0.9820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 44. Train a PCA model and visualize the variance captured by each principal component.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Generate high-dimensional synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Plot variance per component\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', color='#1f77b4')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Variance Captured by Each Principal Component')\n",
        "plt.grid(True)\n",
        "plt.savefig('pca_variance_per_component.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "7jWFy8ine3XK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45. Train a KNN Classifier and perform feature selection before training.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN without feature selection\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train_scaled, y_train)\n",
        "y_pred = knn_clf.predict(X_test_scaled)\n",
        "accuracy_no_selection = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Apply feature selection\n",
        "selector = VarianceThreshold(threshold=0.5)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN with feature selection\n",
        "knn_clf_selected = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = knn_clf_selected.predict(X_test_selected)\n",
        "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "# Print results\n",
        "print(f\"KNN Classifier Accuracy (No Feature Selection): {accuracy_no_selection:.4f}\")\n",
        "print(f\"KNN Classifier Accuracy (With Feature Selection): {accuracy_selected:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELwtimvTe7yi",
        "outputId": "916ff2b0-9cba-40c4-cb16-8a4ab5048b82"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier Accuracy (No Feature Selection): 0.9444\n",
            "KNN Classifier Accuracy (With Feature Selection): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 46. Train a PCA model and visualize the data reconstruction error after reducing dimensions.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Calculate reconstruction error for different components\n",
        "n_components_list = range(1, 11)\n",
        "errors = []\n",
        "\n",
        "for n in n_components_list:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "    error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
        "    errors.append(error)\n",
        "\n",
        "# Plot reconstruction error\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(n_components_list, errors, marker='o', color='#1f77b4')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Reconstruction Error (MSE)')\n",
        "plt.title('PCA Reconstruction Error')\n",
        "plt.grid(True)\n",
        "plt.savefig('pca_reconstruction_error.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "cecphIgne-zB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 47. Train a KNN Classifier and visualize the decision boundary.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset (use first two features for 2D visualization)\n",
        "data = load_iris()\n",
        "X, y = data.data[:, :2], data.target  # Sepal length and sepal width\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_scaled, y)\n",
        "\n",
        "# Create mesh grid for decision boundary\n",
        "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict on mesh grid\n",
        "Z = knn_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary and data points\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, edgecolor='k', cmap='viridis')\n",
        "plt.xlabel('Sepal Length (Scaled)')\n",
        "plt.ylabel('Sepal Width (Scaled)')\n",
        "plt.title('KNN Classifier Decision Boundary (k=5)')\n",
        "plt.savefig('knn_decision_boundary.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "RtDEeNiZfHG6"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 48 Train a PCA model and analyze the effect of different numbers of components on data variance\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA with all components\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Analyze cumulative explained variance for different numbers of components\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Print explained variance for each number of components\n",
        "print(\"Effect of Number of Components on Cumulative Explained Variance:\")\n",
        "for i, variance in enumerate(cumulative_variance, 1):\n",
        "    print(f\"Components: {i}, Cumulative Variance Explained: {variance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsN9NCBmfNjC",
        "outputId": "0468ea55-1c48-4c54-d8ee-20f0a1438849"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effect of Number of Components on Cumulative Explained Variance:\n",
            "Components: 1, Cumulative Variance Explained: 0.1512\n",
            "Components: 2, Cumulative Variance Explained: 0.2449\n",
            "Components: 3, Cumulative Variance Explained: 0.3176\n",
            "Components: 4, Cumulative Variance Explained: 0.3832\n",
            "Components: 5, Cumulative Variance Explained: 0.4413\n",
            "Components: 6, Cumulative Variance Explained: 0.4943\n",
            "Components: 7, Cumulative Variance Explained: 0.5469\n",
            "Components: 8, Cumulative Variance Explained: 0.5985\n",
            "Components: 9, Cumulative Variance Explained: 0.6490\n",
            "Components: 10, Cumulative Variance Explained: 0.6988\n",
            "Components: 11, Cumulative Variance Explained: 0.7463\n",
            "Components: 12, Cumulative Variance Explained: 0.7932\n",
            "Components: 13, Cumulative Variance Explained: 0.8385\n",
            "Components: 14, Cumulative Variance Explained: 0.8817\n",
            "Components: 15, Cumulative Variance Explained: 0.9232\n",
            "Components: 16, Cumulative Variance Explained: 0.9596\n",
            "Components: 17, Cumulative Variance Explained: 0.9816\n",
            "Components: 18, Cumulative Variance Explained: 1.0000\n",
            "Components: 19, Cumulative Variance Explained: 1.0000\n",
            "Components: 20, Cumulative Variance Explained: 1.0000\n"
          ]
        }
      ]
    }
  ]
}