{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 1. What is Boosting in Machine Learning?\n",
        "\n",
        "Boosting is an ensemble machine learning technique that combines multiple weak learners (models that perform slightly better than random guessing, e.g., shallow decision trees) to create a strong predictive model. Unlike standalone models, boosting trains weak learners sequentially, where each learner focuses on correcting the errors of its predecessors, improving overall accuracy.\n",
        "\n",
        "- **Key Idea**: Boosting reduces bias and variance by iteratively learning from mistakes, assigning higher weights to misclassified or poorly predicted instances.\n",
        "- **Example**: AdaBoost, Gradient Boosting, XGBoost, and CatBoost are popular boosting algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. How does Boosting differ from Bagging?\n",
        "\n",
        "Boosting and Bagging are both ensemble techniques, but they differ significantly in their approach. Since you’ve explored Bagging Classifiers and Regressors (e.g., with Decision Trees, SVM, Logistic Regression), here’s a direct comparison:\n",
        "\n",
        "| **Aspect**              | **Bagging**                                                                 | **Boosting**                                                                |\n",
        "|-------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|\n",
        "| **Full Form**           | Bootstrap Aggregating                                                | Iterative improvement by focusing on errors                                |\n",
        "| **Training**            | Trains multiple models independently in parallel on random bootstrap samples. | Trains models sequentially, with each model learning from previous errors. |\n",
        "| **Instance Weighting**  | All instances have equal weight; uses random sampling with replacement.      | Adjusts weights of instances, giving more focus to misclassified ones.      |\n",
        "| **Model Dependency**    | Models are independent; no interaction between them.                        | Models are dependent; each corrects the mistakes of the previous ones.      |\n",
        "| **Objective**           | Reduces variance by averaging (e.g., Random Forest, Bagging Classifier).     | Reduces bias and variance by focusing on hard-to-predict instances.         |\n",
        "| **Example**             | Bagging Classifier with Decision Trees (as in your previous tasks).         | AdaBoost, Gradient Boosting, XGBoost.                                      |\n",
        "\n",
        "- **Connection to Your Work**: In your Bagging Classifier tasks (e.g., using Decision Trees or SVM), models were trained independently on bootstrap samples and combined via majority voting or averaging. Boosting, however, would adjust weights to focus on misclassified instances, potentially improving accuracy on difficult cases.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. What is the key idea behind AdaBoost?\n",
        "\n",
        "**AdaBoost** (Adaptive Boosting) is a boosting algorithm that combines weak learners (typically decision stumps, i.e., single-level decision trees) into a strong classifier by focusing on misclassified instances.\n",
        "\n",
        "- **Key Idea**: AdaBoost assigns weights to training instances, initially equal. After each weak learner is trained, it increases the weights of misclassified instances and decreases the weights of correctly classified ones. Subsequent learners focus more on the harder instances. The final prediction is a weighted combination of all weak learners’ predictions, where each learner’s weight depends on its accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Explain the working of AdaBoost with an example.\n",
        "\n",
        "**How AdaBoost Works**:\n",
        "1. **Initialize Weights**: Assign equal weights to all training instances (e.g., for \\( N \\) instances, each has weight \\( 1/N \\)).\n",
        "2. **Train Weak Learner**: Train a weak learner (e.g., a decision stump) on the weighted dataset.\n",
        "3. **Compute Error**: Calculate the weighted error rate of the learner.\n",
        "4. **Assign Learner Weight**: Compute the learner’s weight based on its error (lower error → higher weight).\n",
        "5. **Update Instance Weights**: Increase weights for misclassified instances and decrease for correctly classified ones, then normalize weights.\n",
        "6. **Repeat**: Train subsequent learners on the updated weights, iterating for a fixed number of rounds or until convergence.\n",
        "7. **Final Prediction**: Combine weak learners’ predictions via a weighted majority vote (classification) or weighted average (regression).\n",
        "\n",
        "**Example**:\n",
        "Suppose you’re classifying Iris flowers (Setosa vs. Non-Setosa) using AdaBoost with decision stumps:\n",
        "- **Dataset**: 10 samples, initially each with weight \\( 1/10 = 0.1 \\).\n",
        "- **Step 1**: Train a decision stump (e.g., “If petal length > 2.5, predict Non-Setosa”). It misclassifies 3 instances.\n",
        "- **Step 2**: Compute the stump’s error (weighted error = 0.3) and assign it a weight (e.g., \\( \\alpha = 0.5 \\cdot \\ln((1-0.3)/0.3) \\approx 0.423 \\)).\n",
        "- **Step 3**: Increase weights of the 3 misclassified instances (e.g., from 0.1 to 0.15) and decrease others, then normalize.\n",
        "- **Step 4**: Train the next stump on the new weights, focusing on the misclassified instances.\n",
        "- **Step 5**: Repeat for, say, 10 stumps.\n",
        "- **Final Model**: Predict by summing the weighted votes of all stumps (e.g., a sample gets a score based on each stump’s prediction and weight).\n",
        "\n",
        "**Output**: The final model is more accurate than a single stump, as it emphasizes harder-to-classify samples.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "\n",
        "**Gradient Boosting** is a boosting algorithm that builds an ensemble of weak learners (typically decision trees) sequentially, where each learner corrects the errors of the previous ones by minimizing a loss function using gradient descent.\n",
        "\n",
        "- **How It Works**: Instead of adjusting instance weights like AdaBoost, Gradient Boosting fits each new learner to the *residual errors* (negative gradients of the loss function) of the previous ensemble’s predictions. The final prediction is the sum of all learners’ predictions, scaled by a learning rate.\n",
        "\n",
        "**Differences from AdaBoost**:\n",
        "| **Aspect**              | **AdaBoost**                                                       | **Gradient Boosting**                                              |\n",
        "|-------------------------|--------------------------------------------------------------------|--------------------------------------------------------------------|\n",
        "| **Error Correction**    | Adjusts instance weights based on misclassifications.              | Fits new learners to residuals (negative gradients) of the loss.   |\n",
        "| **Loss Function**       | Implicitly uses exponential loss for classification.               | Explicitly optimizes a user-defined loss (e.g., MSE, log loss).    |\n",
        "| **Learner Weighting**   | Weights learners based on their error rate.                        | Scales learners’ contributions with a learning rate.               |\n",
        "| **Flexibility**         | Primarily for classification, less flexible loss functions.        | Supports various loss functions for regression and classification. |\n",
        "| **Example**             | Focuses on misclassified points (e.g., in Iris classification).    | Fits trees to residuals (e.g., in California Housing regression). |\n",
        "\n",
        "- **Connection to Your Work**: In your Bagging Regressor tasks (e.g., with Decision Trees), you averaged independent trees to reduce variance. Gradient Boosting, unlike Bagging, would sequentially fit trees to residuals, reducing bias and potentially achieving lower MSE.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. What is the loss function in Gradient Boosting?\n",
        "\n",
        "The loss function in Gradient Boosting measures the error between predicted and actual values, guiding the optimization process. Each new weak learner is trained to minimize this loss by following its negative gradient.\n",
        "\n",
        "- **Common Loss Functions**:\n",
        "  - **Regression**:\n",
        "    - **Mean Squared Error (MSE)**: \\( L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2 \\). Used in your Bagging Regressor tasks for California Housing.\n",
        "    - **Mean Absolute Error (MAE)**: \\( L(y, \\hat{y}) = |y - \\hat{y}| \\).\n",
        "  - **Classification**:\n",
        "    - **Log Loss (Logistic Loss)**: For binary classification, \\( L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})] \\).\n",
        "    - **Exponential Loss**: Used implicitly in AdaBoost, also available in Gradient Boosting.\n",
        "  - **Custom Losses**: Gradient Boosting allows custom loss functions, as long as they are differentiable.\n",
        "\n",
        "- **Role**: The loss function determines the residuals (negative gradients) that each new learner fits. For example, in regression with MSE, residuals are \\( y - \\hat{y} \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "\n",
        "**XGBoost** (Extreme Gradient Boosting) is an optimized implementation of Gradient Boosting with several enhancements:\n",
        "\n",
        "1. **Regularization**: Adds L1 (Lasso) and L2 (Ridge) regularization to penalize complex models, reducing overfitting.\n",
        "2. **Second-Order Optimization**: Uses second-order derivatives (Hessian) of the loss function for faster convergence, unlike traditional Gradient Boosting’s first-order approach.\n",
        "3. **Parallelization**: Supports parallel tree construction, making it faster than traditional Gradient Boosting.\n",
        "4. **Handling Missing Values**: Automatically learns how to handle missing data by assigning them to the best branch.\n",
        "5. **Tree Pruning**: Uses a “max_depth” parameter and prunes trees greedily, stopping when splits no longer improve the loss.\n",
        "6. **Weighted Quantile Sketch**: Efficiently handles large datasets by approximating split points.\n",
        "7. **Sparsity Awareness**: Optimizes for sparse data (e.g., one-hot encoded features).\n",
        "8. **Cross-Validation**: Built-in support for cross-validation to prevent overfitting.\n",
        "\n",
        "- **Connection to Your Work**: Compared to your Bagging Regressor (e.g., MSE of ~0.2558 on California Housing), XGBoost typically achieves lower MSE due to its sequential learning and regularization, especially on complex datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. What is the difference between XGBoost and CatBoost?\n",
        "\n",
        "| **Aspect**              | **XGBoost**                                                                 | **CatBoost**                                                               |\n",
        "|-------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
        "| **Categorical Features** | Requires manual encoding (e.g., one-hot encoding) for categorical data.     | Natively handles categorical features using ordered target encoding.       |\n",
        "| **Training Speed**      | Fast due to parallelization, but encoding categorical data adds overhead.   | Often faster for datasets with categorical features due to native handling.|\n",
        "| **Overfitting Control** | Uses L1/L2 regularization and tree pruning.                                | Uses symmetric trees and ordered boosting to reduce overfitting.           |\n",
        "| **Ease of Use**         | Requires more preprocessing for categorical data.                          | Simpler to use with categorical data; less preprocessing needed.           |\n",
        "| **Performance**         | Strong on numerical data; may overfit with small datasets.                 | Robust for mixed data types; performs well on small datasets.              |\n",
        "| **Implementation**      | Open-source, widely used, with GPU support.                                | Open-source, with GPU support, optimized for categorical data.             |\n",
        "\n",
        "- **Connection to Your Work**: If you used a dataset with categorical features in your Bagging tasks, CatBoost would be more efficient than XGBoost, as it avoids manual encoding.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. What are some real-world applications of Boosting techniques?\n",
        "\n",
        "Boosting techniques like AdaBoost, Gradient Boosting, XGBoost, and CatBoost are widely used in real-world applications due to their high accuracy:\n",
        "1. **Finance**: Credit scoring, fraud detection (e.g., XGBoost for detecting fraudulent transactions).\n",
        "2. **Healthcare**: Disease prediction (e.g., CatBoost for predicting cancer risk from medical records).\n",
        "3. **Marketing**: Customer churn prediction, recommendation systems (e.g., Gradient Boosting for predicting user preferences).\n",
        "4. **Natural Language Processing**: Text classification, sentiment analysis (e.g., XGBoost for spam detection).\n",
        "5. **Retail**: Sales forecasting, inventory optimization (e.g., Gradient Boosting for demand prediction, similar to your California Housing regression tasks).\n",
        "6. **Competitions**: Kaggle and other machine learning competitions, where XGBoost and CatBoost often dominate due to their performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. How does regularization help in XGBoost?\n",
        "\n",
        "Regularization in XGBoost prevents overfitting by adding penalties to the loss function, discouraging overly complex models:\n",
        "- **L1 Regularization (Lasso)**: Adds the absolute value of leaf weights (\\( \\sum |w| \\)) to the loss, promoting sparsity (fewer non-zero weights).\n",
        "- **L2 Regularization (Ridge)**: Adds the squared value of leaf weights (\\( \\sum w^2 \\)) to the loss, penalizing large weights to smooth predictions.\n",
        "- **Benefits**:\n",
        "  - Reduces overfitting by constraining tree complexity.\n",
        "  - Improves generalization to unseen data.\n",
        "  - Stabilizes predictions on noisy datasets (e.g., unlike your Bagging Regressor, which relies solely on averaging to reduce variance).\n",
        "- **Parameters**: Controlled by `lambda` (L2) and `alpha` (L1) in XGBoost.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. What are some hyperparameters to tune in Gradient Boosting models?\n",
        "\n",
        "Key hyperparameters to tune in Gradient Boosting models (e.g., Gradient Boosting, XGBoost, CatBoost):\n",
        "1. **n_estimators**: Number of boosting iterations (trees). Higher values improve performance but increase computation (e.g., you used 10 in Bagging tasks; 100–1000 is common for boosting).\n",
        "2. **learning_rate**: Scales each tree’s contribution. Lower values (e.g., 0.01–0.1) improve accuracy but require more trees.\n",
        "3. **max_depth**: Maximum depth of each tree. Deeper trees (e.g., 3–10) capture complex patterns but risk overfitting.\n",
        "4. **min_child_weight** (XGBoost): Minimum sum of instance weights in a leaf. Higher values prevent overfitting.\n",
        "5. **subsample**: Fraction of samples used per tree (e.g., 0.5–1.0). Similar to `max_samples` in your Bagging tasks but applied sequentially.\n",
        "6. **colsample_bytree**: Fraction of features used per tree (e.g., 0.5–1.0), similar to Random Forest’s feature randomness.\n",
        "7. **lambda/alpha** (XGBoost): L2/L1 regularization strength to control overfitting.\n",
        "8. **depth** (CatBoost): Controls tree depth, similar to `max_depth`.\n",
        "9. **iterations** (CatBoost): Equivalent to `n_estimators`.\n",
        "\n",
        "- **Tuning Strategy**: Use grid search or random search with cross-validation (like in your Bagging cross-validation task) to find optimal values.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. What is the concept of Feature Importance in Boosting?\n",
        "\n",
        "**Feature Importance** in boosting measures how much each feature contributes to the model’s predictions:\n",
        "- **How It’s Calculated**:\n",
        "  - **Gain**: The improvement in loss (e.g., MSE in regression) from splits using a feature, averaged across all trees (used in XGBoost).\n",
        "  - **Weight**: Number of times a feature is used in splits (used in Gradient Boosting).\n",
        "  - **Cover**: Number of samples affected by splits on a feature (used in XGBoost).\n",
        "- **Interpretation**: Higher importance indicates a feature is more influential in reducing the loss or improving predictions.\n",
        "- **Use Cases**: Feature selection, understanding model behavior, and identifying key predictors (e.g., in your California Housing tasks, median income might have high importance).\n",
        "\n",
        "- **Connection to Your Work**: Unlike Bagging, where feature importance is less straightforward, boosting models like XGBoost provide clear importance scores to analyze which features drive predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 13. Why is CatBoost efficient for categorical data?\n",
        "\n",
        "**CatBoost** (Categorical Boosting) is designed to handle categorical features efficiently:\n",
        "1. **Native Categorical Handling**: Uses ordered target encoding, converting categories to numerical values based on target statistics (e.g., mean target value per category), avoiding manual one-hot encoding required in XGBoost.\n",
        "2. **Ordered Boosting**: Reduces overfitting by using a permutation-based approach to calculate target statistics, ensuring unbiased gradient estimates.\n",
        "3. **Symmetric Trees**: Builds balanced trees, reducing memory usage and speeding up predictions compared to traditional Gradient Boosting.\n",
        "4. **Robust to Overfitting**: Handles high-cardinality categorical features (e.g., many unique values) without exploding feature dimensionality.\n",
        "5. **Efficiency**: Faster training and prediction for datasets with categorical features, as it skips preprocessing steps.\n",
        "\n",
        "- **Connection to Your Work**: If your Bagging Classifier tasks involved datasets with categorical features, CatBoost would simplify preprocessing and potentially outperform Bagging by leveraging categorical data effectively.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zxP2rc43W4ok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBDpoA1OWvtr",
        "outputId": "8223fddc-4ee5-4d47-df28-e74f23cbab9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost Classifier\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"AdaBoost Classifier Accuracy: {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost Regressor\n",
        "adaboost_reg = AdaBoostRegressor(\n",
        "    estimator=DecisionTreeRegressor(max_depth=3, random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "adaboost_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MAE\n",
        "y_pred = adaboost_reg.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print MAE\n",
        "print(\"AdaBoost Regressor Mean Absolute Error: {:.4f}\".format(mae))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqC2CvDtXbmc",
        "outputId": "221c41f5-084d-4844-cb4d-5c110959f986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Regressor Mean Absolute Error: 0.6498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance.\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': gb_clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance\n",
        "print(\"Gradient Boosting Classifier Feature Importance:\")\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzT5JZ6oXuUL",
        "outputId": "295dde92-a902-4115-e03c-2d27a1c29d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classifier Feature Importance:\n",
            "                    Feature  Importance\n",
            "7       mean concave points    0.450528\n",
            "27     worst concave points    0.240103\n",
            "20             worst radius    0.075589\n",
            "22          worst perimeter    0.051408\n",
            "21            worst texture    0.039886\n",
            "23               worst area    0.038245\n",
            "1              mean texture    0.027805\n",
            "26          worst concavity    0.018725\n",
            "16          concavity error    0.013068\n",
            "13               area error    0.008415\n",
            "10             radius error    0.006870\n",
            "24         worst smoothness    0.004811\n",
            "19  fractal dimension error    0.004224\n",
            "11            texture error    0.003604\n",
            "5          mean compactness    0.002996\n",
            "15        compactness error    0.002511\n",
            "4           mean smoothness    0.002467\n",
            "17     concave points error    0.002038\n",
            "28           worst symmetry    0.001478\n",
            "12          perimeter error    0.001157\n",
            "6            mean concavity    0.000922\n",
            "18           symmetry error    0.000703\n",
            "14         smoothness error    0.000556\n",
            "8             mean symmetry    0.000520\n",
            "25        worst compactness    0.000450\n",
            "3                 mean area    0.000425\n",
            "2            mean perimeter    0.000201\n",
            "29  worst fractal dimension    0.000175\n",
            "9    mean fractal dimension    0.000107\n",
            "0               mean radius    0.000013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score.\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Regressor\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate R² score\n",
        "y_pred = gb_reg.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print R² score\n",
        "print(\"Gradient Boosting Regressor R² Score: {:.4f}\".format(r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob9y6LwDXyja",
        "outputId": "600b278c-3cfb-44cc-f0f7-5abdd3525964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R² Score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting.\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "gb_pred = gb_clf.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "# Initialize and train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "xgb_pred = xgb_clf.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Gradient Boosting Classifier Accuracy: {:.4f}\".format(gb_accuracy))\n",
        "print(\"XGBoost Classifier Accuracy: {:.4f}\".format(xgb_accuracy))"
      ],
      "metadata": {
        "id": "38kguX9iX5_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7c864d-7afa-402e-f5e6-eaa567dc72e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classifier Accuracy: 0.9561\n",
            "XGBoost Classifier Accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [12:59:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. Train a CatBoost Classifier and evaluate using F1-score.\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "cat_clf = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate F1-score\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Print F1-score\n",
        "print(\"CatBoost Classifier F1-Score (macro): {:.4f}\".format(f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "36Jn6PhCYOod",
        "outputId": "662eb821-525f-47ce-8bcb-d58820e1a626"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0fe6d9d47518>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 19. Train a CatBoost Classifier and evaluate using F1-score.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE).\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Regressor\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MSE\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print MSE\n",
        "print(\"XGBoost Regressor Mean Squared Error: {:.4f}\".format(mse))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zSJ9reoYcbm",
        "outputId": "25f41796-f381-4561-ff93-1c0cd1a45629"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Regressor Mean Squared Error: 0.2226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Train an AdaBoost Classifier and visualize feature importance.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost Classifier\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': adaboost_clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance\n",
        "print(\"AdaBoost Classifier Feature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='#1f77b4')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('AdaBoost Classifier Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.savefig('adaboost_feature_importance.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9LMXba2YfLL",
        "outputId": "81a65897-5205-4ae4-9bbb-b175171ad447"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Feature Importance:\n",
            "             Feature  Importance\n",
            "3   petal width (cm)    0.423293\n",
            "2  petal length (cm)    0.374844\n",
            "1   sepal width (cm)    0.159977\n",
            "0  sepal length (cm)    0.041886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Train a Gradient Boosting Regressor and plot learning curves.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Compute learning curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    gb_reg, X, y, cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# Calculate mean and std\n",
        "train_scores_mean = -np.mean(train_scores, axis=1)\n",
        "test_scores_mean = -np.mean(test_scores, axis=1)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_sizes, train_scores_mean, label='Training MSE', color='#1f77b4')\n",
        "plt.plot(train_sizes, test_scores_mean, label='Validation MSE', color='#ff7f0e')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Gradient Boosting Regressor Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('gb_learning_curves.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "qlLYDdzsYlMr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Train an XGBoost Classifier and visualize feature importance.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': xgb_clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance\n",
        "print(\"XGBoost Classifier Feature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='#1f77b4')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('XGBoost Classifier Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.savefig('xgb_feature_importance.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qyjELngYrEL",
        "outputId": "a78d1433-3fec-4803-9839-64c15aa6cb33"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:07:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classifier Feature Importance:\n",
            "                    Feature  Importance\n",
            "27     worst concave points    0.285641\n",
            "7       mean concave points    0.235738\n",
            "22          worst perimeter    0.174253\n",
            "20             worst radius    0.075987\n",
            "23               worst area    0.057031\n",
            "21            worst texture    0.021676\n",
            "26          worst concavity    0.018667\n",
            "12          perimeter error    0.018553\n",
            "1              mean texture    0.014433\n",
            "0               mean radius    0.012819\n",
            "3                 mean area    0.012336\n",
            "16          concavity error    0.011444\n",
            "13               area error    0.008527\n",
            "6            mean concavity    0.007317\n",
            "10             radius error    0.005298\n",
            "24         worst smoothness    0.005140\n",
            "19  fractal dimension error    0.004664\n",
            "4           mean smoothness    0.004639\n",
            "11            texture error    0.004528\n",
            "15        compactness error    0.004081\n",
            "9    mean fractal dimension    0.003935\n",
            "28           worst symmetry    0.003510\n",
            "14         smoothness error    0.003320\n",
            "18           symmetry error    0.003301\n",
            "5          mean compactness    0.002019\n",
            "17     concave points error    0.001146\n",
            "8             mean symmetry    0.000000\n",
            "2            mean perimeter    0.000000\n",
            "25        worst compactness    0.000000\n",
            "29  worst fractal dimension    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Train a CatBoost Classifier and plot the confusion matrix.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "cat_clf = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute confusion matrix\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('CatBoost Classifier Confusion Matrix')\n",
        "plt.savefig('catboost_confusion_matrix.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "9xkX8NHxZf6s",
        "outputId": "eb38230c-e34c-4267-d7af-84dd18161449"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ab7f189e4c20>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy.\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Test different numbers of estimators\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = []\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    adaboost_clf = AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    adaboost_clf.fit(X_train, y_train)\n",
        "    y_pred = adaboost_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"AdaBoost Classifier (n_estimators={n}) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "3tIJsZzcZkd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Train a Gradient Boosting Classifier and visualize the ROC curve.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = gb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='#1f77b4', label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Gradient Boosting Classifier ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('gb_roc_curve.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "5pWVTGhMZ5ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "xgb_reg = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 0.3]}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(xgb_reg, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and predict\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Learning Rate: {grid_search.best_params_['learning_rate']}\")\n",
        "print(f\"XGBoost Regressor Mean Squared Error: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8WOxWwFZ_0j",
        "outputId": "6530047f-400d-461a-c2db-9ae6042af7dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Learning Rate: 0.3\n",
            "XGBoost Regressor Mean Squared Error: 0.2226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting.\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Generate imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost without class weights\n",
        "cat_clf = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "f1_no_weight = f1_score(y_test, y_pred)\n",
        "\n",
        "# Train CatBoost with class weights\n",
        "cat_clf_weighted = CatBoostClassifier(iterations=100, auto_class_weights='Balanced', random_state=42, verbose=0)\n",
        "cat_clf_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = cat_clf_weighted.predict(X_test)\n",
        "f1_weighted = f1_score(y_test, y_pred_weighted)\n",
        "\n",
        "# Print F1-scores\n",
        "print(f\"CatBoost Classifier F1-Score (No Weights): {f1_no_weight:.4f}\")\n",
        "print(f\"CatBoost Classifier F1-Score (Balanced Weights): {f1_weighted:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "fphaaq2paHKD",
        "outputId": "f735916a-a2c9-4130-f35b-66013ee74a2a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-65e93a9bc477>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Train an AdaBoost Classifier and analyze the effect of different learning rates.\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Test different learning rates\n",
        "learning_rates = [0.1, 0.5, 1.0, 2.0]\n",
        "\n",
        "# Store accuracies\n",
        "for lr in learning_rates:\n",
        "    adaboost_clf = AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "        n_estimators=50,\n",
        "        learning_rate=lr,\n",
        "        random_state=42\n",
        "    )\n",
        "    adaboost_clf.fit(X_train, y_train)\n",
        "    y_pred = adaboost_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"AdaBoost Classifier (learning_rate={lr}) Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsimmTmmaN5b",
        "outputId": "ebed9c00-0d4c-48df-cf5a-2e8ce2b8ad7f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier (learning_rate=0.1) Accuracy: 1.0000\n",
            "AdaBoost Classifier (learning_rate=0.5) Accuracy: 0.9667\n",
            "AdaBoost Classifier (learning_rate=1.0) Accuracy: 0.9333\n",
            "AdaBoost Classifier (learning_rate=2.0) Accuracy: 0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, objective='multi:softprob', eval_metric='mlogloss', random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and calculate log-loss\n",
        "y_pred_proba = xgb_clf.predict_proba(X_test)\n",
        "logloss = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "# Print log-loss\n",
        "print(\"XGBoost Classifier Log-Loss: {:.4f}\".format(logloss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLMXrfZaV9D",
        "outputId": "f5390859-c0cd-4781-acb2-d8bf0e1ab040"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classifier Log-Loss: 0.0093\n"
          ]
        }
      ]
    }
  ]
}